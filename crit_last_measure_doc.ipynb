{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import common\n",
    "from data_reader import PostPdfDocReader, Document\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "#29s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сборник всех статей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Alexandra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "100%|██████████| 2454/2454 [01:16<00:00, 32.17it/s]\n",
      "100%|██████████| 2454/2454 [00:09<00:00, 257.12it/s]\n"
     ]
    }
   ],
   "source": [
    "articles = list(PostPdfDocReader().read_saved_as_document(\"upd\"))\n",
    "filtered_articles = common.filter_common_words(articles, min_freq=0, max_freq=0.65)\n",
    "\n",
    "#~4m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Делим на группы по известным категориям"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "components_known = dict()\n",
    "for doc in filtered_articles:\n",
    "    category = doc.name.split('-')[0]\n",
    "    if not category in components_known:\n",
    "        components_known[category] = []\n",
    "    components_known[category].append(doc.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scoring\n",
    "\n",
    "#50s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для этих разделённых по категориям кластеров ищем топики `components_topics_known`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "62it [00:02, 25.84it/s]\n"
     ]
    }
   ],
   "source": [
    "components_docs_known = [\n",
    "    Document(k, sum((doc_tokens for doc_tokens in docs_tokens), []))\n",
    "    for k, docs_tokens in enumerate(components_known.values())\n",
    "]\n",
    "component_to_topics_known = scoring.get_topics_ctfidf(components_docs_known, reduce_frequent_words=True, bm25_weighting=True, top_k=10, min_df=0.3, max_df=0.8)\n",
    "components_topics_known = [v for k, v in component_to_topics_known.items()]\n",
    "\n",
    "#1m1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Геофизика\n",
      "top 10: ['землетрясение', 'осадок', 'юг', 'географический', 'геология', 'река', 'лёд', 'климатический', 'тёплый', 'сток']\n",
      "similarity: 0.19677855153050688\n"
     ]
    }
   ],
   "source": [
    "print(list(components_known.keys())[11])\n",
    "print(\"top 10:\", components_topics_known[11])\n",
    "print(\"similarity:\", common.compute_top_words_sim([components_topics_known[11]], w2v_model, topn=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('w2v_model.pickle', 'rb') as file:\n",
    "    w2v_model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19677855153050688"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X -- топик группы, y -- индекс группы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "i = 0\n",
    "for gr in components_topics_known:\n",
    "    for word in gr:\n",
    "        X.append(gr)\n",
    "        y.append(i)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Берем модель для подсчёта расстояний между словами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.models import KeyedVectors\n",
    "# w2v_model = KeyedVectors.load_word2vec_format('cc.ru.300.vec.gz', binary=False)\n",
    "\n",
    "#21m10s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "\n",
    "def calinski_harabasz_index(topics, components, model):\n",
    "    topic_words_count = 0\n",
    "    cluster_means = []\n",
    "    topic_words = []\n",
    "    components_counts = []\n",
    "    for i in range(len(topics)):\n",
    "        topic = topics[i]\n",
    "        words = []\n",
    "        for word in topic:\n",
    "            try:\n",
    "                words.append(model[word])\n",
    "                topic_words_count += 1\n",
    "            except KeyError:\n",
    "                continue\n",
    "        if len(words) > 0:\n",
    "            cluster_means.append(np.array(words).mean(axis=0))\n",
    "            topic_words.append(words)\n",
    "            components_counts.append(len(components[i]))\n",
    "    overall_mean = np.array(cluster_means).mean(axis=0)\n",
    "    bcss = 0\n",
    "    wcss = 0\n",
    "    for i in range(len(topic_words)):\n",
    "        cluster_norm = LA.norm(cluster_means[i] - overall_mean)\n",
    "        bcss += components_counts[i] * cluster_norm * cluster_norm\n",
    "        for word in topic_words[i]:\n",
    "            word_norm = LA.norm(cluster_means[i] - word)\n",
    "            wcss += word_norm * word_norm\n",
    "            \n",
    "    clusters_count = len(cluster_means)\n",
    "    return (bcss * (topic_words_count - clusters_count)) / (wcss * (clusters_count - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.992782219165669"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calinski_harabasz_index(components_topics_known, list(components_known.values()), w2v_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bigrams-Louvain + Calinski–Harabasz index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('full_graph.pickle', 'rb') as f:\n",
    "    full_graph = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import community.community_louvain as community_louvain #python-louvain\n",
    "\n",
    "partition = community_louvain.best_partition(full_graph)\n",
    "\n",
    "communities = {}\n",
    "for node, community_id in partition.items():\n",
    "    if community_id not in communities:\n",
    "        communities[community_id] = [node]\n",
    "    else:\n",
    "        communities[community_id].append(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_name_to_doc = {doc.name: doc for doc in filtered_articles}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [00:16,  1.81s/it]\n"
     ]
    }
   ],
   "source": [
    "communities_docs = [\n",
    "    Document(k, sum((doc_name_to_doc[doc_name].tokens for doc_name in docs_names), []))\n",
    "    for k, docs_names in communities.items()\n",
    "]\n",
    "communities_to_topics = scoring.get_topics_ctfidf(communities_docs, reduce_frequent_words=True, bm25_weighting=False, top_k=10, min_df=0.1, max_df=0.85)\n",
    "communities_topics = [v for k, v in communities_to_topics.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75.10279899210188"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calinski_harabasz_index(communities_topics, list(communities.values()), w2v_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "elib + word2vec + calinski_harabasz_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2454it [00:17, 138.02it/s]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import tqdm\n",
    "\n",
    "\n",
    "def get_topn_words(docs, top_k=10):\n",
    "    texts = [' '.join(doc.tokens) for doc in docs]\n",
    "    tfidf = TfidfVectorizer(max_features=8_000, min_df=0.1, max_df=0.65)\n",
    "    transformed = tfidf.fit_transform(texts)\n",
    "\n",
    "    doc_to_top_words = {}\n",
    "    for doc, row in tqdm.tqdm(zip(docs, transformed)):\n",
    "        most_scored_idx = np.argsort(-row.toarray())[0, :top_k]\n",
    "        top_words = tfidf.get_feature_names_out()[most_scored_idx]\n",
    "        scores = row.toarray()[0, most_scored_idx]\n",
    "        doc_to_top_words[doc.name] = (list(top_words), scores)\n",
    "\n",
    "    return doc_to_top_words\n",
    "\n",
    "doc_to_topn_words = get_topn_words(articles, top_k=100)\n",
    "\n",
    "#25s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2454/2454 [00:05<00:00, 485.83it/s]\n"
     ]
    }
   ],
   "source": [
    "doc_to_vec = {}\n",
    "for doc in tqdm.tqdm(articles):\n",
    "    top_words, scores = doc_to_topn_words[doc.name]\n",
    "    vectorized_doc = [token for token in top_words if token in w2v_model]\n",
    "    if not vectorized_doc:\n",
    "        continue\n",
    "    doc_to_vec[doc.name] = w2v_model[vectorized_doc].mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2454it [02:28, 16.51it/s] \n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "from common import get_graph\n",
    "import networkx as nx\n",
    "\n",
    "def cosine_sim(first_doc, second_doc, doc_to_vec):\n",
    "    first = doc_to_vec[first_doc.name]\n",
    "    second = doc_to_vec[second_doc.name]\n",
    "\n",
    "    return first @ second / np.linalg.norm(first) / np.linalg.norm(second)\n",
    "\n",
    "\n",
    "graph = get_graph(filtered_articles, 0.95, partial(cosine_sim, doc_to_vec=doc_to_vec))\n",
    "doc_name_to_doc = {doc.name: doc for doc in articles}\n",
    "\n",
    "components = list(nx.connected_components(graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "154it [00:03, 49.25it/s]\n"
     ]
    }
   ],
   "source": [
    "components_docs = [\n",
    "    Document(k, sum((doc_name_to_doc[doc_name].tokens for doc_name in docs_names), []))\n",
    "    for k, docs_names in enumerate(components)\n",
    "]\n",
    "component_to_topics = scoring.get_topics_ctfidf(components_docs, reduce_frequent_words=True, bm25_weighting=False, top_k=10, min_df=0.1, max_df=0.85)\n",
    "components_topics = [v for k, v in component_to_topics.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.6375640464664003"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calinski_harabasz_index(components_topics, components, w2v_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "elib + jaccard + calinski_harabasz_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "with open('jac_graph.pickle', 'rb') as f:\n",
    "    graph = pickle.load(f)\n",
    "doc_name_to_doc = {doc.name: doc for doc in articles}\n",
    "\n",
    "components = list(nx.connected_components(graph))\n",
    "len(components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16it [00:00, 77.13it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['специалист',\n",
       " 'профессиональный',\n",
       " 'подготовка',\n",
       " 'стандарт',\n",
       " 'обслуживание',\n",
       " 'образовательный',\n",
       " 'учебный',\n",
       " 'предприятие',\n",
       " 'требование',\n",
       " 'качество']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "components_docs = [\n",
    "    Document(k, sum((doc_name_to_doc[doc_name].tokens for doc_name in docs_names), []))\n",
    "    for k, docs_names in enumerate(components)\n",
    "]\n",
    "component_to_topics = scoring.get_topics_ctfidf(components_docs, reduce_frequent_words=True, bm25_weighting=False, top_k=10, min_df=0.1, max_df=0.85)\n",
    "components_topics = [v for k, v in component_to_topics.items()]\n",
    "component_to_topics[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3190372095349591"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calinski_harabasz_index(components_topics, components, w2v_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "elib + bert + ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "sent_model = SentenceTransformer('multi-qa-mpnet-base-dot-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# with open('elib_tops_bert_doc_to_embed.pickle', 'wb') as file:\n",
    "#     pickle.dump(doc_to_embed, file)\n",
    "\n",
    "with open('elib_tops_bert_doc_to_embed.pickle', 'rb') as file:\n",
    "    doc_to_embed = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 768)\n"
     ]
    }
   ],
   "source": [
    "for (k, v) in doc_to_embed.items():\n",
    "    print(v.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common import cosine_sim, get_graph\n",
    "from functools import partial\n",
    "import networkx as nx\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 100 is different from 768)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 13\u001b[0m\n\u001b[0;32m      9\u001b[0m             graph\u001b[38;5;241m.\u001b[39madd_edge(second_doc\u001b[38;5;241m.\u001b[39mname, first_doc\u001b[38;5;241m.\u001b[39mname, weight\u001b[38;5;241m=\u001b[39msim)\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m graph\n\u001b[1;32m---> 13\u001b[0m graph \u001b[38;5;241m=\u001b[39m \u001b[43mget_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiltered_articles\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcosine_sim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdoc_to_vec\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoc_to_embed\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m doc_name_to_doc \u001b[38;5;241m=\u001b[39m {doc\u001b[38;5;241m.\u001b[39mname: doc \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m filtered_articles}\n\u001b[0;32m     17\u001b[0m components \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(nx\u001b[38;5;241m.\u001b[39mconnected_components(graph))\n",
      "Cell \u001b[1;32mIn[18], line 5\u001b[0m, in \u001b[0;36mget_graph\u001b[1;34m(docs, threshold, sim_fn)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m first_ix, first_doc \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(\u001b[38;5;28menumerate\u001b[39m(docs)):\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m second_ix, second_doc \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(docs[first_ix \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m:]):\n\u001b[1;32m----> 5\u001b[0m         sim \u001b[38;5;241m=\u001b[39m \u001b[43msim_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfirst_doc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msecond_doc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m sim \u001b[38;5;241m<\u001b[39m threshold:\n\u001b[0;32m      7\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Alexandra\\Desktop\\TextsGraphCompare\\common.py:105\u001b[0m, in \u001b[0;36mcosine_sim\u001b[1;34m(first_doc, second_doc, doc_to_vec)\u001b[0m\n\u001b[0;32m    102\u001b[0m first \u001b[38;5;241m=\u001b[39m doc_to_vec[first_doc\u001b[38;5;241m.\u001b[39mname]\n\u001b[0;32m    103\u001b[0m second \u001b[38;5;241m=\u001b[39m doc_to_vec[second_doc\u001b[38;5;241m.\u001b[39mname]\n\u001b[1;32m--> 105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfirst\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msecond\u001b[49m \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(first) \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(second)\n",
      "\u001b[1;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 100 is different from 768)"
     ]
    }
   ],
   "source": [
    "def get_graph(docs, threshold, sim_fn):\n",
    "    graph = nx.Graph()\n",
    "    for first_ix, first_doc in tqdm.tqdm(enumerate(docs)):\n",
    "        for second_ix, second_doc in enumerate(docs[first_ix + 1:]):\n",
    "            sim = sim_fn(first_doc, second_doc)\n",
    "            if sim < threshold:\n",
    "                continue\n",
    "            graph.add_edge(first_doc.name, second_doc.name, weight=sim)\n",
    "            graph.add_edge(second_doc.name, first_doc.name, weight=sim)\n",
    "\n",
    "    return graph\n",
    "\n",
    "graph = get_graph(filtered_articles, 0.8, partial(cosine_sim, doc_to_vec=doc_to_embed))\n",
    "\n",
    "doc_name_to_doc = {doc.name: doc for doc in filtered_articles}\n",
    "\n",
    "components = list(nx.connected_components(graph))\n",
    "len(components)\n",
    "components_docs = [\n",
    "    Document(k, sum((doc_name_to_doc[doc_name].tokens for doc_name in docs_names), []))\n",
    "    for k, docs_names in enumerate(components)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m component_to_topics \u001b[38;5;241m=\u001b[39m \u001b[43mscoring\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_topics_ctfidf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcomponents_docs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce_frequent_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbm25_weighting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_df\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_df\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.8\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m components_topics \u001b[38;5;241m=\u001b[39m [v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m component_to_topics\u001b[38;5;241m.\u001b[39mitems()]\n",
      "File \u001b[1;32mc:\\Users\\Alexandra\\Desktop\\TextsGraphCompare\\scoring.py:22\u001b[0m, in \u001b[0;36mget_topics_ctfidf\u001b[1;34m(components, top_k, min_df, max_df, max_features, reduce_frequent_words, bm25_weighting)\u001b[0m\n\u001b[0;32m     20\u001b[0m texts_sentences \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mstr\u001b[39m(t) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m component\u001b[38;5;241m.\u001b[39mtokens]) \u001b[38;5;28;01mfor\u001b[39;00m component \u001b[38;5;129;01min\u001b[39;00m components]\n\u001b[0;32m     21\u001b[0m count_vectorizer \u001b[38;5;241m=\u001b[39m CountVectorizer(min_df\u001b[38;5;241m=\u001b[39mmin_df, max_df\u001b[38;5;241m=\u001b[39mmax_df)\n\u001b[1;32m---> 22\u001b[0m sentences_cv \u001b[38;5;241m=\u001b[39m \u001b[43mcount_vectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts_sentences\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m ctfidf \u001b[38;5;241m=\u001b[39m ClassTfidfTransformer(bm25_weighting, reduce_frequent_words)\n\u001b[0;32m     25\u001b[0m sentences_ctfidf \u001b[38;5;241m=\u001b[39m ctfidf\u001b[38;5;241m.\u001b[39mfit_transform(sentences_cv)\n",
      "File \u001b[1;32mc:\\Users\\Alexandra\\Desktop\\TextsGraphCompare\\env\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1328\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1320\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1321\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1322\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1323\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1324\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1325\u001b[0m             )\n\u001b[0;32m   1326\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1328\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1331\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Alexandra\\Desktop\\TextsGraphCompare\\env\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1218\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1216\u001b[0m     vocabulary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(vocabulary)\n\u001b[0;32m   1217\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vocabulary:\n\u001b[1;32m-> 1218\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1219\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1220\u001b[0m         )\n\u001b[0;32m   1222\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indptr[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39miinfo(np\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mmax:  \u001b[38;5;66;03m# = 2**31 - 1\u001b[39;00m\n\u001b[0;32m   1223\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _IS_32BIT:\n",
      "\u001b[1;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "component_to_topics = scoring.get_topics_ctfidf(components_docs, reduce_frequent_words=True, bm25_weighting=True, top_k=10, min_df=0.3, max_df=0.8)\n",
    "components_topics = [v for k, v in component_to_topics.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calinski_harabasz_index(components_topics, components, w2v_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ГОСТ + Jaccard "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1162it [27:05,  1.40s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 25\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# set your path to a zipfile\u001b[39;00m\n\u001b[0;32m     24\u001b[0m zipfile_path \u001b[38;5;241m=\u001b[39m Path()\u001b[38;5;241m.\u001b[39mcwd() \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdocx35.zip\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, text \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(_read_docx_from_zipfile(zipfile_path)):\n\u001b[0;32m     26\u001b[0m     doc_name \u001b[38;5;241m=\u001b[39m Path(name)\u001b[38;5;241m.\u001b[39mwith_suffix(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mname\n\u001b[0;32m     27\u001b[0m     docs[doc_name] \u001b[38;5;241m=\u001b[39m text\n",
      "File \u001b[1;32mc:\\Users\\Alexandra\\Desktop\\TextsGraphCompare\\env\\lib\\site-packages\\tqdm\\std.py:1182\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1179\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1181\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1182\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1185\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[25], line 17\u001b[0m, in \u001b[0;36m_read_docx_from_zipfile\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m archive\u001b[38;5;241m.\u001b[39mopen(name) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m     15\u001b[0m     doc \u001b[38;5;241m=\u001b[39m docx\u001b[38;5;241m.\u001b[39mDocument(file)\n\u001b[1;32m---> 17\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(p\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39msplit()) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m doc\u001b[38;5;241m.\u001b[39mparagraphs \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(p\u001b[38;5;241m.\u001b[39mtext) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m name, text\n",
      "Cell \u001b[1;32mIn[25], line 17\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m archive\u001b[38;5;241m.\u001b[39mopen(name) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m     15\u001b[0m     doc \u001b[38;5;241m=\u001b[39m docx\u001b[38;5;241m.\u001b[39mDocument(file)\n\u001b[1;32m---> 17\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[38;5;241m.\u001b[39msplit()) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m doc\u001b[38;5;241m.\u001b[39mparagraphs \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(p\u001b[38;5;241m.\u001b[39mtext) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m name, text\n",
      "File \u001b[1;32mc:\\Users\\Alexandra\\Desktop\\TextsGraphCompare\\env\\lib\\site-packages\\docx\\text\\paragraph.py:165\u001b[0m, in \u001b[0;36mParagraph.text\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtext\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    153\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"The textual content of this paragraph.\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \n\u001b[0;32m    155\u001b[0m \u001b[38;5;124;03m    The text includes the visible-text portion of any hyperlinks in the paragraph.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;124;03m    is preserved. All run-level formatting, such as bold or italic, is removed.\u001b[39;00m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 165\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_p\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Alexandra\\Desktop\\TextsGraphCompare\\env\\lib\\site-packages\\docx\\oxml\\text\\paragraph.py:102\u001b[0m, in \u001b[0;36mCT_P.text\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;129m@property\u001b[39m  \u001b[38;5;66;03m# pyright: ignore[reportIncompatibleVariableOverride]\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtext\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     97\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"The textual content of this paragraph.\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    Inner-content child elements like `w:r` and `w:hyperlink` are translated to\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m    their text equivalent.\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxpath\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mw:r | w:hyperlink\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Alexandra\\Desktop\\TextsGraphCompare\\env\\lib\\site-packages\\docx\\oxml\\text\\paragraph.py:102\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;129m@property\u001b[39m  \u001b[38;5;66;03m# pyright: ignore[reportIncompatibleVariableOverride]\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtext\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     97\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"The textual content of this paragraph.\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    Inner-content child elements like `w:r` and `w:hyperlink` are translated to\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m    their text equivalent.\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mxpath(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw:r | w:hyperlink\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\Alexandra\\Desktop\\TextsGraphCompare\\env\\lib\\site-packages\\docx\\oxml\\text\\run.py:124\u001b[0m, in \u001b[0;36mCT_R.text\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtext\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    117\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"The textual content of this run.\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \n\u001b[0;32m    119\u001b[0m \u001b[38;5;124;03m    Inner-content child elements like `w:tab` are translated to their text\u001b[39;00m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;124;03m    equivalent.\u001b[39;00m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[0;32m    123\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e)\n\u001b[1;32m--> 124\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxpath\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mw:br | w:cr | w:noBreakHyphen | w:ptab | w:t | w:tab\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    125\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Alexandra\\Desktop\\TextsGraphCompare\\env\\lib\\site-packages\\docx\\oxml\\xmlchemy.py:712\u001b[0m, in \u001b[0;36mBaseOxmlElement.xpath\u001b[1;34m(self, xpath_str)\u001b[0m\n\u001b[0;32m    707\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mxpath\u001b[39m(\u001b[38;5;28mself\u001b[39m, xpath_str: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    708\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Override of `lxml` _Element.xpath() method.\u001b[39;00m\n\u001b[0;32m    709\u001b[0m \n\u001b[0;32m    710\u001b[0m \u001b[38;5;124;03m    Provides standard Open XML namespace mapping (`nsmap`) in centralized location.\u001b[39;00m\n\u001b[0;32m    711\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 712\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxpath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxpath_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnamespaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnsmap\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile, re, docx\n",
    "import json\n",
    "import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "def _read_docx_from_zipfile(path):\n",
    "    archive = zipfile.ZipFile(path, 'r')\n",
    "\n",
    "    for name in archive.namelist():\n",
    "        if not re.fullmatch(r'.*\\.docx', name):\n",
    "            continue\n",
    "\n",
    "        with archive.open(name) as file:\n",
    "            doc = docx.Document(file)\n",
    "            \n",
    "            text = '.'.join([' '.join(p.text.split()) for p in doc.paragraphs if len(p.text) > 0])\n",
    "\n",
    "        yield name, text\n",
    "\n",
    "\n",
    "docs = {}\n",
    "# set your path to a zipfile\n",
    "zipfile_path = Path().cwd() / 'data' / 'docx35.zip'\n",
    "for name, text in tqdm.tqdm(_read_docx_from_zipfile(zipfile_path)):\n",
    "    doc_name = Path(name).with_suffix('.txt').name\n",
    "    docs[doc_name] = text\n",
    "\n",
    "#1162 за 27m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1162"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_path = Path().cwd() / 'data'\n",
    "# data_path.mkdir(exist_ok=True)\n",
    "# with open(data_path / 'docs.json', 'w') as f:\n",
    "#     json.dump(docs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('data/docs.json') as f:\n",
    "    raw_docs = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_reader import JsonDocReader, Document\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Alexandra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1162it [47:04,  2.43s/it]\n"
     ]
    }
   ],
   "source": [
    "# data_reader = JsonDocReader(Path('data/docs.json')).read_documents()\n",
    "# docs = list(data_reader)\n",
    "\n",
    "# with open('data/preprocessed_docs.json', 'w') as f:\n",
    "#     docs_dict = {doc.name: ' '.join(doc.tokens) for doc in docs}\n",
    "#     json.dump(docs_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1162/1162 [00:08<00:00, 133.70it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('data/preprocessed_docs.json') as f:\n",
    "    docs = json.load(f)\n",
    "    docs = [Document(k, v.split()) for k, v in docs.items()]\n",
    "filtered_docs = common.filter_common_words(docs, min_freq=0.1, max_freq=0.65)\n",
    "doc_name_to_doc = {doc.name: doc for doc in docs}\n",
    "filtered_docs.remove(Document('gost_r_54481-2011.txt', []))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1161it [03:36,  5.36it/s]\n"
     ]
    }
   ],
   "source": [
    "graph = common.get_graph(filtered_docs, 0.85, partial(common.jaccard_sim, _words_cache={}))\n",
    "doc_name_to_doc = {doc.name: doc for doc in docs}\n",
    "\n",
    "components = list(nx.connected_components(graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:00, 51.84it/s]\n"
     ]
    }
   ],
   "source": [
    "components_docs = [\n",
    "    Document(k, sum((doc_name_to_doc[doc_name].tokens for doc_name in docs_names), []))\n",
    "    for k, docs_names in enumerate(components)\n",
    "]\n",
    "component_to_topics = scoring.get_topics_ctfidf(components_docs, reduce_frequent_words=True, bm25_weighting=False, top_k=10, min_df=0.1, max_df=0.85)\n",
    "components_topics = [v for k, v in component_to_topics.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19883913752157242"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common.compute_top_words_sim(components_topics, w2v_model, topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тестировочный черновик"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic=components_topics_known[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = w2v_model[topic[0], topic[1], topic[2], topic[3], topic[4],\n",
    "          topic[5], topic[6], topic[7], topic[8], topic[9]].mean(axis=0)\n",
    "b = [a, a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8206531"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy import linalg as LA\n",
    "\n",
    "LA.norm(np.array(b).mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
